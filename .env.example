# .env.example file to be copied to .evn file and adapted to your needs

# set log level (DEBUG, INFO, WARNING, ERROR, CRITICAL). This can be overriden on the 
# command line (-v = INFO, -vv = DEBUG). Default is WARNING.
LOG_LEVEL=INFO

# input.po file context
# The input file itself. Usually provided on the command line but can also be set in the .env
# INPUT_PO=tests/input/input.po
# The output file. Usually provided on the command line but can also be set in the .env. Use auto_po_lyglot -h to see
# how the file name is computed if not provided.
# OUTPUT_PO=tests/output/output.po

# Primary language (msgids). Can be overriden on the command line
ORIGINAL_LANGUAGE=English
# And translation language (msgstrs). Can be overriden on the command line
CONTEXT_LANGUAGE=French

# Set the LLM client, can be openai, ollama, claude or claude_cached. Default is ollama.Can be overriden on the command line
# LLM_CLIENT=ollama
# Set the model, must be consistent with the LLM client. Leave undefined to use the default model for the client.
# Default values are for the ollama client: llama3.1:8b, openai: gpt-4o-2024-08-06 and claude:claude-3-5-sonnet-20240620
# Can be overriden on the command line
# LLM_MODEL="gemma2:2b"

# Depending on the LLM provider you chose, give below the proper API_KEY. No key needed for Ollama (free)
# Note these values will override the ones in the environment if they exist so put in comment if you want to use
# the ones in the environment.
# for OpenAI models, set:
OPENAI_API_KEY=<your key>
# for Claude models, set:
ANTHROPIC_API_KEY=<your key>

# OLLAMA server URL when used with OpenAI API; The default value is for the Ollama local server
# There is no command line argument for this setting, so if your server does not run locally, please change it
OLLAMA_BASE_URL="http://localhost:11434/v1"

# the target languages to test for translation. Give a list of comma separated languages
# Can be overriden on the command line (only one laguage in this case)
TARGET_LANGUAGES=Italian,Spanish,German,Portuguese

# One prebuilt system and user prompts are provided by default in `default_prompts.py`. If you want, you can create
# below your own system and user prompts. The system prompt can use the following placeholders:
# {original_language}, {context_language}, {target_language}, {simple_original_phrase}, {simple_context_translation}, 
# {simple_target_translation}, {ambiguous_original_phrase}, {ambiguous_context_translation}, {ambiguous_target_translation},
# {ambiguous_explanation}, {po_placeholder_original_phrase_1}, {po_placeholder_original_phrase_2}, {po_placeholder_original_phrase_3},
# {po_placeholder_context_translation_1}, {po_placeholder_context_translation_2}, {po_placeholder_context_translation_3}, 
# {po_placeholder_target_translation_1}, {po_placeholder_target_translation_2}, {po_placeholder_target_translation_3}.
# (all phrases, explanations and translations are taken from the examples below), 
#SYSTEM_PROMPT="You are a highly skilled translator with expertise in {original_language}, {context_language}, and {target_language}..."
# The user prompt can use only the following placeholders: {original_language}, {original_phrase}, {context_language}, {context_translation},
# also taken from the examples below.
#USER_PROMPT="{original_language} sentence: \"{original_phrase}\", {context_language} translation: \"{context_translation}\""
